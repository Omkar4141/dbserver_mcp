

# ğŸ’° Expense Tracker using MCP (FastMCP + LangChain + Ollama)- Sample Project for understanding MCP 

This project demonstrates a **simple end-to-end MCP (Model Context Protocol)** example where:

* A **FastMCP server** exposes tools to manage expenses stored in **SQLite**
* A **LangChain client** connects to the MCP server
* An **LLM (Llama 3.2 via Ollama)** decides when to call tools
* Natural language queries like

  > *"Add my expense 500 to groceries"*
  > automatically trigger backend database operations


## ğŸ“Œ Architecture Overview

```
User (CLI)
   â”‚
   â–¼
LangChain Client (client.py)
   â”‚
   â”‚  MCP (stdio)
   â–¼
FastMCP Server (main.py)
   â”‚
   â–¼
SQLite Database (expenses.db)
```

### Key Components

| Component                 | Description                            |
| ------------------------- | -------------------------------------- |
| **FastMCP**               | Exposes database operations as tools   |
| **LangChain MCP Adapter** | Connects LLM to MCP tools              |
| **Ollama (Llama 3.2:3b)** | Interprets user intent and calls tools |
| **SQLite**                | Persistent expense storage             |

---

## ğŸ“‚ Project Structure

```
.
â”œâ”€â”€ main.py        # FastMCP expense database server
â”œâ”€â”€ client.py      # LangChain MCP client with LLM
â”œâ”€â”€ expenses.db    # SQLite database (auto-created)
â””â”€â”€ README.md
```

## ğŸš€ Features

* âœ… Add expenses using natural language
* âœ… View total expenses
* âœ… List all expenses
* âœ… Automatic tool selection by LLM
* âœ… Persistent storage using SQLite
* âœ… MCP-compliant architecture


## ğŸ› ï¸ Tools Exposed by MCP Server

The FastMCP server exposes the following tools:

### `add_expense`

Adds a new expense entry.

```json
{
  "amount": 500,
  "category": "groceries",
  "description": "weekly shopping"
}
```

### `get_total`

Returns the total sum of all expenses.


### `get_all_expenses`

Returns a list of all recorded expenses.


## âš™ï¸ Prerequisites

Make sure you have the following installed:

* **Python 3.10+**
* **Ollama**
* **Llama 3.2 model**
* **uv** (Python package runner)

```bash
ollama pull llama3.2:3b
```

## ğŸ“¦ Install Dependencies

```bash
uv add fastmcp langchain langchain-mcp-adapters langchain-ollama
```

## â–¶ï¸ Running the Client

Update paths inside `client.py`:

```python
"command": "/home/omkar/.local/bin/uv",
"args": [
    "run",
    "fastmcp",
    "run",
    "/full/path/to/main.py"
]
```

Then run:

```bash
uv run client.py
```

## ğŸ§  How It Works (Step-by-Step)

1. User enters a natural language query
2. LLM decides whether a tool is needed
3. If required:

   * Tool name + arguments are generated
4. LangChain invokes MCP tool
5. Result is returned to LLM
6. LLM generates final user-friendly respons




Just tell me ğŸ‘
